{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# smart-open"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "`smart_open` is a Python 3 library for efficient streaming of very large files from/to storages such as S3, GCS, Azure Blob Storage, HDFS, WebHDFS, HTTP, HTTPS, SFTP, or local filesystem. It supports transparent, on-the-fly (de-)compression for a variety of different formats.\n",
    "\n",
    "`smart_open` is a drop-in replacement for Python’s built-in `open()`: it can do anything `open` can (100% compatible, falls back to native `open` wherever possible), plus lots of nifty extra stuff on top."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install 'smart_open[s3]'\n",
    "# or pip install smart_open[s3] if not using zsh because zsh uses [] for globbing\n",
    "# also available [gpc] and [azure]"
   ]
  },
  {
   "source": [
    "See [smart_open project site](https://pypi.org/project/smart-open/) and the [help file online](https://github.com/RaRe-Technologies/smart_open/blob/master/help.txt) for more details. Also availabe built-in with `help('smart_open')`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help('smart_open')"
   ]
  },
  {
   "source": [
    "`smart_open` uses `boto3` to talk to S3. So it uses the same mechanisms as `boto3` to authenticate credentials. If `aws_cli` is set up in your system (and your credentials are stored in your env vars), then you can access your S3 system without the need to include your credentials. Very useful for running notebooks:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from smart_open import open\n",
    "\n",
    "with open(\"s3://audantic-data-test/attom/avm_staging/ds=20160623/AVM_20160623_001.txt.gz\") as f:\n",
    "    line_count = 0\n",
    "    for line in f:\n",
    "        if line_count > 2:\n",
    "            break\n",
    "        print(line)\n",
    "        line_count += 1\n"
   ]
  },
  {
   "source": [
    "Otherwise, you can pass a `boto3` session, or specify the credentials within the S3 URI"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, boto3\n",
    "\n",
    "session = boto3.Session(\n",
    "     aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "     aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'],\n",
    ")\n",
    "\n",
    "with open(\n",
    "    \"s3://audantic-data-test/attom/avm_staging/ds=20160623/AVM_20160623_001.txt.gz\",\n",
    "    transport_params={\"session\": session},    \n",
    ") as f:\n",
    "    line_count = 0\n",
    "    for line in f:\n",
    "        if line_count > 2:\n",
    "            break\n",
    "        print(line)\n",
    "        line_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_access_key_id = os.environ['AWS_ACCESS_KEY_ID']\n",
    "aws_secret_access_key = os.environ['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "with open(\n",
    "    f\"s3://{aws_access_key_id}:{aws_secret_access_key}@audantic-data-test/attom/avm_staging/ds=20160623/AVM_20160623_001.txt.gz\",\n",
    "    transport_params={\"session\": session},    \n",
    ") as f:\n",
    "    line_count = 0\n",
    "    for line in f:\n",
    "        if line_count > 2:\n",
    "            break\n",
    "        print(line)\n",
    "        line_count += 1"
   ]
  },
  {
   "source": [
    "`smart_open` natively reads and writes gzip and bzip2 files over HTTP, S3 and other protocols, based on the file extension. Support for other file extensions and compression formats can be easily added"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"s3://audantic-test/smart_open-test/smart_open_test.gz\", \"w\") as f:\n",
    "    f.write(\"Here we are\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"s3://audantic-data-test/attom/avm_staging/ds=20160623/AVM_20160623_001.txt.gz\") as fin:\n",
    "    with open(\"s3://audantic-test/smart_open-test/smart_open_test2.bzip2\", \"w\") as fout:\n",
    "        line_count = 0\n",
    "        for line in fin:\n",
    "            fout.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(open)"
   ]
  },
  {
   "source": [
    "## Iterating Over an S3 Bucket’s Contents\n",
    "Since going over all (or select) keys in an S3 bucket is a very common operation, there’s also an extra function `smart_open.s3.iter_bucket()` that does this efficiently, processing the bucket keys in parallel (using multiprocessing):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smart_open import s3\n",
    "help(s3.iter_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"audantic-data\"\n",
    "prefix = \"attom/foreclosure_staging/\"\n",
    "for key, content in s3.iter_bucket(\n",
    "    bucket, prefix=prefix, \n",
    "    accept_key=lambda key: \"/ds=202009\" in key, \n",
    "    workers=2,\n",
    "    key_limit=3, \n",
    "):\n",
    "    print(key, len(content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
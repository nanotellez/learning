{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "surprising-conflict",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Apache Arrow\n",
    "Apache Arrow is a development platform for in-memory analytics. It contains a set of technologies that enable big data systems to process and move data fast. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-galaxy",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"arrow_desc.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-industry",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"columnar_storage.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "considerable-allocation",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "key = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "secret = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "\n",
    "spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.2,com.amazonaws:aws-java-sdk-pom:1.10.34\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", key)\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-front",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Converting to Pandas is expensive\n",
    "DataFrame to Pandas in PySpark process collects all rows to the Spark driver, serializes each row into Python’s pickle format (row by row) and sends them to a Python worker process. At the end of this converting procedure, it unpickles each row into a massive list of tuples that is then processed into Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lyric-telling",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- x: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "df = spark.range(1 << 22).toDF(\"id\").withColumn(\"x\", rand())\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "controlled-coordination",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.4 s, sys: 930 ms, total: 12.3 s\n",
      "Wall time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "%time pdf = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "changing-jungle",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.194304e+06</td>\n",
       "      <td>4.194304e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.097152e+06</td>\n",
       "      <td>4.999261e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.210791e+06</td>\n",
       "      <td>2.886626e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.208270e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>2.500795e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.097152e+06</td>\n",
       "      <td>4.998171e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.145727e+06</td>\n",
       "      <td>7.500119e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.194303e+06</td>\n",
       "      <td>9.999998e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id             x\n",
       "count  4.194304e+06  4.194304e+06\n",
       "mean   2.097152e+06  4.999261e-01\n",
       "std    1.210791e+06  2.886626e-01\n",
       "min    0.000000e+00  3.208270e-07\n",
       "25%    1.048576e+06  2.500795e-01\n",
       "50%    2.097152e+06  4.998171e-01\n",
       "75%    3.145727e+06  7.500119e-01\n",
       "max    4.194303e+06  9.999998e-01"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-denial",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now enable arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "contained-chorus",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "arrow_spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.2,com.amazonaws:aws-java-sdk-pom:1.10.34\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "arrow_spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", key)\n",
    "arrow_spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "primary-doubt",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "arrow_spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "amber-salvation",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- x: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arrow_df = arrow_spark.range(1 << 22).toDF(\"id\").withColumn(\"x\", rand())\n",
    "arrow_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "muslim-singapore",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 239 ms, sys: 117 ms, total: 356 ms\n",
      "Wall time: 840 ms\n"
     ]
    }
   ],
   "source": [
    "%time pdf = arrow_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "powerful-county",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.194304e+06</td>\n",
       "      <td>4.194304e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.097152e+06</td>\n",
       "      <td>5.003103e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.210791e+06</td>\n",
       "      <td>2.886706e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.304829e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.048576e+06</td>\n",
       "      <td>2.502151e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.097152e+06</td>\n",
       "      <td>5.004541e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.145727e+06</td>\n",
       "      <td>7.503815e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.194303e+06</td>\n",
       "      <td>9.999999e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id             x\n",
       "count  4.194304e+06  4.194304e+06\n",
       "mean   2.097152e+06  5.003103e-01\n",
       "std    1.210791e+06  2.886706e-01\n",
       "min    0.000000e+00  1.304829e-07\n",
       "25%    1.048576e+06  2.502151e-01\n",
       "50%    2.097152e+06  5.004541e-01\n",
       "75%    3.145727e+06  7.503815e-01\n",
       "max    4.194303e+06  9.999999e-01"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-nightmare",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Also from pandas to spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "furnished-liquid",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.001093</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>0.002214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000809</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>0.999185</td>\n",
       "      <td>0.999939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.720892</td>\n",
       "      <td>-4.530946</td>\n",
       "      <td>-4.749366</td>\n",
       "      <td>-4.922817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.676655</td>\n",
       "      <td>-0.674001</td>\n",
       "      <td>-0.674134</td>\n",
       "      <td>-0.672791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.001742</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>0.001910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.674406</td>\n",
       "      <td>0.676717</td>\n",
       "      <td>0.674769</td>\n",
       "      <td>0.676591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.499123</td>\n",
       "      <td>4.702640</td>\n",
       "      <td>4.966137</td>\n",
       "      <td>4.516064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    A               B               C               D\n",
       "count  1000000.000000  1000000.000000  1000000.000000  1000000.000000\n",
       "mean        -0.001093        0.001170        0.000832        0.002214\n",
       "std          1.000809        0.999986        0.999185        0.999939\n",
       "min         -4.720892       -4.530946       -4.749366       -4.922817\n",
       "25%         -0.676655       -0.674001       -0.674134       -0.672791\n",
       "50%         -0.001742        0.001121        0.001066        0.001910\n",
       "75%          0.674406        0.676717        0.674769        0.676591\n",
       "max          5.499123        4.702640        4.966137        4.516064"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import util\n",
    "\n",
    "pd.util.testing.N = 10**6\n",
    "test_df = util.testing.makeDataFrame()\n",
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "excited-straight",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.2 s, sys: 203 ms, total: 25.4 s\n",
      "Wall time: 25 s\n"
     ]
    }
   ],
   "source": [
    "%time sdf = spark.createDataFrame(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "answering-understanding",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- A: double (nullable = true)\n",
      " |-- B: double (nullable = true)\n",
      " |-- C: double (nullable = true)\n",
      " |-- D: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "strong-supplier",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.001093</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>0.002214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000809</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>0.999185</td>\n",
       "      <td>0.999939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.720892</td>\n",
       "      <td>-4.530946</td>\n",
       "      <td>-4.749366</td>\n",
       "      <td>-4.922817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.676655</td>\n",
       "      <td>-0.674001</td>\n",
       "      <td>-0.674134</td>\n",
       "      <td>-0.672791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.001742</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>0.001910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.674406</td>\n",
       "      <td>0.676717</td>\n",
       "      <td>0.674769</td>\n",
       "      <td>0.676591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.499123</td>\n",
       "      <td>4.702640</td>\n",
       "      <td>4.966137</td>\n",
       "      <td>4.516064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    A               B               C               D\n",
       "count  1000000.000000  1000000.000000  1000000.000000  1000000.000000\n",
       "mean        -0.001093        0.001170        0.000832        0.002214\n",
       "std          1.000809        0.999986        0.999185        0.999939\n",
       "min         -4.720892       -4.530946       -4.749366       -4.922817\n",
       "25%         -0.676655       -0.674001       -0.674134       -0.672791\n",
       "50%         -0.001742        0.001121        0.001066        0.001910\n",
       "75%          0.674406        0.676717        0.674769        0.676591\n",
       "max          5.499123        4.702640        4.966137        4.516064"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "known-lawsuit",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 199 ms, sys: 25.6 ms, total: 224 ms\n",
      "Wall time: 251 ms\n"
     ]
    }
   ],
   "source": [
    "os.environ['ARROW_PRE_0_15_IPC_FORMAT']='1'\n",
    "%time asdf = arrow_spark.createDataFrame(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "partial-mandate",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- A: double (nullable = true)\n",
      " |-- B: double (nullable = true)\n",
      " |-- C: double (nullable = true)\n",
      " |-- D: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "asdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-computer",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But...\n",
    "Why would we convert a massive spark data frame to pandas? We would lose the parallelized capabilities and all computations would be ridiculously slow anyway. That's what koalas is for, isn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-chester",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## I'm glad you asked!\n",
    "Enter... **Pandas Used Defined Functions (UDFs)** a.k.a. Vectorized UDFs\n",
    ">Pandas UDFs are user defined functions that are executed by Spark using Arrow to transfer data and Pandas to work with the data. A Pandas UDF is defined using the keyword pandas_udf as a decorator or to wrap the function, no additional configuration is required. Currently, there are two types of Pandas UDF: Scalar and Grouped Map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "given-delay",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+--------+\n",
      "| id|                  x|group_id|\n",
      "+---+-------------------+--------+\n",
      "|  0| 0.7835014960581982|       9|\n",
      "|  1| 0.6304803124378475|       8|\n",
      "|  2| 0.8438476250450363|       4|\n",
      "|  3| 0.5897122285716005|       6|\n",
      "|  4|0.20807418193297245|       3|\n",
      "+---+-------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "new_df = arrow_df.withColumn(\"group_id\", (rand() * 10).cast(IntegerType()))\n",
    "new_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-saskatchewan",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grouped aggregate\n",
    "Defines an aggregation from one or more `pandas.Series` to a scalar value, where each `pandas.Series` represents a column within the group or window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "passive-smell",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "@pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)\n",
    "def mean_udf(val):\n",
    "    os.environ['ARROW_PRE_0_15_IPC_FORMAT']='1'\n",
    "    return val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dying-discipline",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|group_id|        mean_udf(x)|\n",
      "+--------+-------------------+\n",
      "|       1| 0.5003724296015791|\n",
      "|       6| 0.5005115712065953|\n",
      "|       3| 0.5006386456346672|\n",
      "|       5|0.49991454914227007|\n",
      "|       9| 0.4999234648309748|\n",
      "|       4| 0.5000941894368167|\n",
      "|       8| 0.5005127133164889|\n",
      "|       7| 0.5007053770330026|\n",
      "|       2| 0.5000558271576734|\n",
      "|       0| 0.5003753971571799|\n",
      "+--------+-------------------+\n",
      "\n",
      "CPU times: user 26.4 ms, sys: 12.8 ms, total: 39.1 ms\n",
      "Wall time: 2.57 s\n"
     ]
    }
   ],
   "source": [
    "%time new_df.groupby(\"group_id\").agg(mean_udf(new_df['x'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "soviet-diana",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------+\n",
      "| id|                 x|group_id|\n",
      "+---+------------------+--------+\n",
      "|  0|0.4695762895238291|       5|\n",
      "|  1|0.8155342295932761|       1|\n",
      "|  2|0.7769013623312877|       2|\n",
      "|  3|0.7413768141953538|       1|\n",
      "|  4|0.9841248543797412|       4|\n",
      "+---+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+-------------------+\n",
      "|group_id|        mean_udf(x)|\n",
      "+--------+-------------------+\n",
      "|       1|0.49963746215395277|\n",
      "|       6| 0.4992933200746855|\n",
      "|       3| 0.5005319476083719|\n",
      "|       5|0.49978673289073505|\n",
      "|       9| 0.5005034392982204|\n",
      "|       4| 0.5004049064645534|\n",
      "|       8| 0.5001358157567577|\n",
      "|       7|0.49955033307267627|\n",
      "|       2| 0.4999911395441541|\n",
      "|       0| 0.4992863812864833|\n",
      "+--------+-------------------+\n",
      "\n",
      "CPU times: user 18.1 ms, sys: 10.4 ms, total: 28.5 ms\n",
      "Wall time: 3.41 s\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "slow_df = df.withColumn(\"group_id\", (rand() * 10).cast(IntegerType()))\n",
    "slow_df.show(5)\n",
    "%time slow_df.groupby(\"group_id\").agg(mean_udf(slow_df['x'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-corporation",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grouped map\n",
    "Grouped map Pandas UDFs are used with `groupBy().apply()` which implements the “split-apply-combine” pattern. Split-apply-combine consists of three steps:\n",
    "\n",
    "- Split the data into groups by using `DataFrame.groupBy`.\n",
    "- Apply a function on each group. The input and output of the function are both `pandas.DataFrame`. The input data contains all the rows and columns for each group.\n",
    "- Combine the results into a new spark `DataFrame`.\n",
    "\n",
    "To use groupBy().apply(), the user needs to define the following:\n",
    "\n",
    "- A Python function that defines the computation for each group.\n",
    "- A StructType object or a string that defines the schema of the output DataFrame.\n",
    "\n",
    "The column labels of the returned `pandas.DataFrame` must either match the field names in the defined output schema if specified as strings, or match the field data types by position if not strings, e.g. integer indices.\n",
    "\n",
    "Arrow **is supposed to** speed up the steps where each group is turned into a `pandas.DataFrame` and where all the resulting `pandas.Dataframe` objects are combined into a single spark `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "radio-sight",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "@pandas_udf(\"id integer, x double, group_id integer\", PandasUDFType.GROUPED_MAP)\n",
    "def subtract_mean(pdf):\n",
    "    # pdf is a pandas.DataFrame\n",
    "    os.environ['ARROW_PRE_0_15_IPC_FORMAT']='1'\n",
    "    val = pdf.x\n",
    "    return pdf.assign(x=val - val.mean())\n",
    "\n",
    "@pandas_udf(\"id integer, x double, group_id integer\", PandasUDFType.GROUPED_MAP)\n",
    "def make_new(pdf):\n",
    "    os.environ['ARROW_PRE_0_15_IPC_FORMAT']='1'\n",
    "    rows = []\n",
    "    for i in range(len(pdf)):\n",
    "        row = pdf.loc[i].to_dict()\n",
    "        if row[\"x\"] * 10 > row[\"group_id\"]:\n",
    "            new_row = {\n",
    "                \"id\": row[\"id\"],\n",
    "                \"x\": row[\"group_id\"] - row[\"x\"] * 10,\n",
    "                \"group_id\": row[\"group_id\"]\n",
    "            }\n",
    "            rows.append(new_row)\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "collected-reasoning",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------+\n",
      "| id|                   x|group_id|\n",
      "+---+--------------------+--------+\n",
      "|  1| 0.31490414362579955|       1|\n",
      "| 40|  0.4258973148723636|       1|\n",
      "| 62| 0.06216563732985847|       1|\n",
      "| 81| -0.2658185064388757|       1|\n",
      "|100|  0.2423298675086637|       1|\n",
      "|111|-0.16079115290063883|       1|\n",
      "|124|-0.46484840242143033|       1|\n",
      "|126| 0.07271004548102722|       1|\n",
      "|131|  0.4577507022900106|       1|\n",
      "|136|-0.11222963663258179|       1|\n",
      "|145| -0.2587136345611961|       1|\n",
      "|146|  0.2935153178233523|       1|\n",
      "|158|-0.21808939746794265|       1|\n",
      "|161| -0.2760088691222756|       1|\n",
      "|192|  0.3573616534146574|       1|\n",
      "|201|  0.3899767922051003|       1|\n",
      "|206| -0.4839786325336748|       1|\n",
      "|222| -0.3764642765977415|       1|\n",
      "|230| 0.18714294966433942|       1|\n",
      "|232| 0.05346919440626552|       1|\n",
      "+---+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 20.5 ms, sys: 9.41 ms, total: 29.9 ms\n",
      "Wall time: 1.97 s\n"
     ]
    }
   ],
   "source": [
    "%time slow_df.groupby(\"group_id\").apply(subtract_mean).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "english-liechtenstein",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------+\n",
      "| id|                   x|group_id|\n",
      "+---+--------------------+--------+\n",
      "| 39|  0.3941321966706638|       1|\n",
      "| 49|  0.2800457317561573|       1|\n",
      "| 51|  0.4723183196682934|       1|\n",
      "| 56| 0.17780227668203763|       1|\n",
      "| 65|-0.29535415278073907|       1|\n",
      "| 79| 0.13679756438091062|       1|\n",
      "| 80| -0.3112120679074296|       1|\n",
      "| 90|-0.06858036938887069|       1|\n",
      "| 98| -0.4072505871917307|       1|\n",
      "|106|-0.40478885596122094|       1|\n",
      "|141| 0.41695642373563724|       1|\n",
      "|151|-0.02222740680657942|       1|\n",
      "|170| 0.34704178936104824|       1|\n",
      "|171|-0.18128532357799942|       1|\n",
      "|187| -0.2400998967551562|       1|\n",
      "|196|-0.06369698353156272|       1|\n",
      "|205|-0.05135405091547163|       1|\n",
      "|207|  0.3435252598696463|       1|\n",
      "|214|-0.04771468292684111|       1|\n",
      "|222|    -0.3478538644075|       1|\n",
      "+---+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 13.6 ms, sys: 7.6 ms, total: 21.2 ms\n",
      "Wall time: 2.66 s\n"
     ]
    }
   ],
   "source": [
    "%time new_df.groupby(\"group_id\").apply(subtract_mean).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "agreed-tribune",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------+\n",
      "| id|                   x|group_id|\n",
      "+---+--------------------+--------+\n",
      "|  1|  -7.155342295932762|       1|\n",
      "| 40|  -8.265274008398402|       1|\n",
      "| 62|  -4.627957232973351|       1|\n",
      "| 81| -1.3481157952860086|       1|\n",
      "|100|  -6.429599534761403|       1|\n",
      "|111|  -2.398389330668378|       1|\n",
      "|126|  -4.733401314485038|       1|\n",
      "|131|  -8.583807882574872|       1|\n",
      "|136| -2.8840044933489484|       1|\n",
      "|145|  -1.419164514062805|       1|\n",
      "|146|  -6.941454037908288|       1|\n",
      "|158| -1.8254068849953393|       1|\n",
      "|161| -1.2462121684520095|       1|\n",
      "|192|   -7.57991739382134|       1|\n",
      "|201|  -7.906068781725768|       1|\n",
      "|222|-0.24165809369735092|       1|\n",
      "|230|   -5.87773035631816|       1|\n",
      "|232|  -4.540992803737421|       1|\n",
      "|234| -3.9221715265684747|       1|\n",
      "|264|  -7.990430338852571|       1|\n",
      "+---+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 27.4 ms, sys: 13.9 ms, total: 41.3 ms\n",
      "Wall time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "%time slow_df.groupby(\"group_id\").apply(make_new).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "greater-concentration",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+--------+\n",
      "| id|                  x|group_id|\n",
      "+---+-------------------+--------+\n",
      "| 39| -7.943939928444344|       1|\n",
      "| 49| -6.803075279299279|       1|\n",
      "| 51| -8.725801158420639|       1|\n",
      "| 56| -5.780640728558081|       1|\n",
      "| 65|-1.0490764339303147|       1|\n",
      "| 79| -5.370593605546811|       1|\n",
      "| 80|-0.8904972826634092|       1|\n",
      "| 90| -3.316814267848999|       1|\n",
      "|141| -8.172182199094078|       1|\n",
      "|151| -3.780343893671911|       1|\n",
      "|170| -7.473035855348188|       1|\n",
      "|171| -2.189764725957711|       1|\n",
      "|187|-1.6016189941861434|       1|\n",
      "|196|-3.3656481264220783|       1|\n",
      "|205| -3.489077452582989|       1|\n",
      "|207| -7.437870560434169|       1|\n",
      "|214|-3.5254711324692938|       1|\n",
      "|222|-0.5240793176627054|       1|\n",
      "|225|-1.7866853540597125|       1|\n",
      "|239| -6.250230750907675|       1|\n",
      "+---+-------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 19.3 ms, sys: 11.6 ms, total: 31 ms\n",
      "Wall time: 2min 23s\n"
     ]
    }
   ],
   "source": [
    "%time new_df.groupby(\"group_id\").apply(make_new).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dried-involvement",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group_id</th>\n",
       "      <th>id</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   group_id   id    x\n",
       "0       1.0  1.0 -4.0\n",
       "1       1.0  3.0 -5.0\n",
       "2       1.0  5.0 -6.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xxx = pd.DataFrame({\n",
    "    \"id\": [1,2,3,4,5],\n",
    "    \"x\": [0.5, 0.4, 0.6, 0.2, 0.7],\n",
    "    \"group_id\": [1,5,1,5,1]\n",
    "})\n",
    "\n",
    "def pd_make_new(pdf):\n",
    "    rows = []\n",
    "    for i in range(len(pdf)):\n",
    "        row = pdf.loc[i].to_dict()\n",
    "        if row[\"x\"] * 10 > row[\"group_id\"]:\n",
    "            new_row = {\n",
    "                \"id\": row[\"id\"],\n",
    "                \"x\": row[\"group_id\"] - row[\"x\"] * 10,\n",
    "                \"group_id\": row[\"group_id\"]\n",
    "            }\n",
    "            rows.append(new_row)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "pd_make_new(xxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "hired-ethics",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1.0, 'x': 0.5, 'group_id': 1.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = xxx.loc[0]\n",
    "row.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-rendering",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
